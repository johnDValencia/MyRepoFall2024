Model,MMLU,creator,AL score,"Parameters 
(Bn)","Tokens 
trained (B)",Ratio Tokens ,Announced,year,month,date,Lab,Playground,"MMLU
-Pro",GPQA,Link,Archiecture,Note,open access,force label,show only
"source: LifeArchitect
https://docs.google.com/spreadsheets/d/1kc262HZSMAWI6FVsh0zJwbB-ooYvzhCHaHcNUiA0_hY/edit?gid=1158069878#gid=1158069878",,,"ALScore 
""ALScore"" is a quick and dirty rating of the model's power. The formula is:
Sqr Root of (Parameters x Tokens) ÷ 300.
Any ALScore ≥ 1.0 is a powerful model in mid-2023.",,,"Ratio Tokens:Params
(Chinchilla scaling≥20:1)",,,,as numeric,,,,,,,,,,
AMD-Llama-135m,23.0,other,0.0,0.135,670,"4,963:1",Sep/2024,2024,9,5.75,AMD,https://huggingface.co/amd/AMD-Llama-135m,,,https://www.amd.com/en/developer/resources/technical-articles/introducing-amd-first-slm-135m-model-fuels-ai-advancements.html,Dense,"Small language model (SLM) trained on 70,000 open access books",,,
Apple On-Device Jun 24,26.8,other,0.2,3.04,"1,500",494:1,Jun/2024,2024,6,5.50,Apple,https://github.com/apple/corenet/tree/main/projects/openelm,,,https://arxiv.org/abs/2404.14619,Dense,,,YES,significant models
Arctic,67.3,other,4.3,480,"3,500",8:1,Apr/2024,2024,4,5.33,Snowflake AI Research,https://arctic.streamlit.app/,,,https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/,Hybrid,,,,
Atlas,47.9,meta,0.1,11,40,4:1,Aug/2022,2022,8,3.67,Meta AI,,,,https://arxiv.org/abs/2208.03299,Dense,,,,
Baichuan 2,54.2,chinese,0.6,13,"2,600",200:1,Sep/2023,2023,9,4.75,Baichuan Intelligence,,,,https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf,Dense,Chinese open-access equivalent to Meta's Llama model,YES,,
BLOOM ,39.1,other,0.8,176,366,3:1,Jul/2022,2022,7,3.58,BigScience,https://huggingface.co/spaces/huggingface/bloom_demo,,,https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml,Dense,(tr11-176B-ml),YES,,significant models
BloombergGPT,39.2,other,0.6,50,569,12:1,Mar/2023,2023,3,4.25,Bloomberg,,,,https://arxiv.org/abs/2303.17564,Dense,"Finance-focussed (of course), based on BLOOM, underperforms against GPT 3",,,significant models
Bytedance 175B,44,chinese,0.8,175,300,2:1,Feb/2024,2024,2,5.17,ByteDance,,,,https://arxiv.org/abs/2402.15627,Dense,GPT 3-esque,,,
Bytedance 530B,44,chinese,1.3,530,300,1:1,Feb/2024,2024,2,5.17,ByteDance,,,,https://arxiv.org/abs/2402.15627,Dense,GPT 3-esque,,YES,
Chameleon,65.8,meta,1.9,34,"9,200",271:1,May/2024,2024,5,5.42,Meta AI,https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live,,,https://arxiv.org/abs/2405.09818,Dense,,,,
ChatGPT (gpt-3.5-turbo),70.0,openAI,,20,,,Nov/2022,2022,11,3.92,OpenAI,https://chat.openai.com/,,28.1,https://openai.com/blog/chatgpt,Dense,,,,significant models
Chinchilla,67.5,google,1.0,70,"1,400",20:1,Mar/2022,2022,3,3.25,DeepMind,,,,https://arxiv.org/abs/2203.15556,Dense,First to double tokens per size increase,,,
ChuXin,,other,0.2,1.6,"2,300","1,438:1",May/2024,2024,5,5.42,Independent,https://huggingface.co/chuxin-llm/Chuxin-1.6B-Base,,,https://arxiv.org/abs/2405.04828,Dense,ChuXin-1M performs well across all context window lengths up to 1M (!),,,
Claude 2,78.5,anthropic,1.9,130,"2,500",20:1,Jul/2023,2023,8,4.67,Anthropic,https://claude.ai/,,,https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf,Dense,"Expanded input and output length (up to 100,00 tokens) allowing the AI model to analyze long documents such as technical guides or entire books",,,significant models
Claude 2.1,78.5,anthropic,1.9,130,"2,500",20:1,Nov/2023,2023,11,4.92,Anthropic,https://claude.ai/,,,https://www.anthropic.com/index/claude-2-1,Dense,"Fewer hallucinations, 200k context length, tool use",,YES,significant models
Claude 3 Opus,86.8,anthropic,29.8,2000,"40,000",20:1,Mar/2024,2024,3,5.25,Anthropic,https://claude.ai/,68.5,59.5,https://www.anthropic.com/claude-3-model-card,Dense,200K context window! 1M for researchers,,YES,significant models
Claude 3.5 Sonnet*,90.5,anthropic,,,,,Jun/2024,2024,6,5.50,Anthropic,https://poe.com/Claude-3.5-Sonnet,78,65,https://www.anthropic.com/news/claude-3-5-sonnet,Dense,,,YES,significant models
Command-R,37.9,other,0.5,35,700,20:1,Mar/2024,2024,3,5.25,Cohere,Cohere,37.9,,https://txt.cohere.com/command-r/,Dense,"has knowledge outside training set (RAG), tool use",,,
Command-R+,75.7,other,2.1,104,"4,000",39:1,Apr/2024,2024,4,5.33,Cohere,https://huggingface.co/spaces/CohereForAI/c4ai-command-r-plus,,,https://huggingface.co/CohereForAI/c4ai-command-r-plus,Dense,"business orientated - ""purpose-built to excel at real-world enterprise use cases.""",,,
DBRX,73.7,other,4.2,132,"12,000",91:1,Mar/2024,2024,3,5.25,MosaicML,https://huggingface.co/spaces/databricks/dbrx-instruct,,,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,MoE,,,,
DCLM-Baseline 7B 2.6T,63.7,other,0.4,7,"2,600",372:1,Jun/2024,2024,6,5.50,International,https://huggingface.co/apple/DCLM-Baseline-7B,,,https://arxiv.org/abs/2406.11794,Dense,,,,
DeepSeek-V2,54.8,chinese,4.6,236,"8,100",35:1,May/2024,2024,5,5.42,DeepSeek-AI,https://chat.deepseek.com/,54.8,,https://arxiv.org/abs/2405.04434,MoE,"Huge dataset, 12% Chinese ""Therefore, we acknowledge that DeepSeek-V2 still has a slight gap in basic English capabilities with LLaMA3 70B"".",,,
Ernie 3.5,65.1,chinese,,,,,Jul/2023,2023,7,4.58,Baidu,https://yiyan.baidu.com/,,,https://www.reuters.com/technology/chinas-baidu-unveils-latest-version-its-ernie-ai-model-2023-10-17/,Dense,"""Enhanced Representation through kNowledge IntEgration""",,,
Ernie 4.0,86,chinese,14.9,1000,"20,000",20:1,Oct/2023,2023,10,4.83,Baidu,https://yiyan.baidu.com/,,,https://www.reuters.com/technology/chinas-baidu-unveils-latest-version-its-ernie-ai-model-2023-10-17/,Dense,Enhanced Representation through kNowledge IntEgration,,,
Ernie 4.0 Turbo*,86,chinese,14.9,1000,"20,000",20:1,Jun/2024,2024,6,5.50,Baidu,https://www.reuters.com/technology/artificial-intelligence/baidu-launches-upgraded-ai-model-says-user-base-hits-300-mln-2024-06-28/,,,https://www.reuters.com/technology/chinas-baidu-unveils-latest-version-its-ernie-ai-model-2023-10-17/,Dense,"""rivals GPT-4 in capabilities""",,,
EXAONE 3.0,27.4,other,0.8,7.8,"8,000","1,026:1",Aug/2024,2024,8,5.67,LG,https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct,27.4,10.1,https://arxiv.org/abs/2408.03541,Dense,“EXAONE”=“EXpert AI for EveryONE”,,,significant models
Falcon 180B,70.6,other,2.6,180,"3,500",20:1,Sep/2023,2023,9,4.75,TII,https://huggingface.co/spaces/tiiuae/falcon-180b-demo,,,https://arxiv.org/abs/2311.16867,Dense,Largest open-access model,YES,,
Falcon 2 11B,58.4,other,0.8,11,"5,500",500:1,May/2024,2024,5,,TII,https://huggingface.co/tiiuae/falcon-11B,,,https://www.tii.ae/news/falcon-2-uaes-technology-innovation-institute-releases-new-ai-model-series-outperforming-metas,Dense,foundational large language model (LLM) with 40 billion parameters trained on one trillion tokens,,,significant models
Falcon Mamba 7B,62.1,other,0.7,7,"6,000",858:1,Aug/2024,2024,8,5.67,TII,https://falconllm.tii.ae/falcon-models.html,14.47,8.05,https://falconllm.tii.ae/tii-releases-first-sslm-with-falcon-mamba-7b.html,Dense,,,,
Flan-PaLM,73.5,google,2.2,540,780,2:1,Oct/2022,2022,10,3.83,Google,,,,https://arxiv.org/abs/2210.11416,Dense,,,,
Galactica,52.6,meta,0.8,120,450,4:1,Nov/2022,2022,11,3.92,Meta AI,https://galactica.org/,,,https://galactica.org/static/paper.pdf,Dense,scientific only,YES,,significant models
Gemini 1.5 Flash,78.9,google,,,,,May/2024,2024,5,5.42,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,59.1,39.5,https://goo.gle/GeminiV1-5,MoE,1M context length.,,,significant models
Gemini 1.5 Pro,85.9,google,22.4,1500,"30,000",20:1,Feb/2024,2024,2,5.17,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,69,46.2,https://goo.gle/GeminiV1-5,MoE,100 languages,,YES,significant models
Gemini Ultra 1.0,83.7,google,22.4,1500,"30,000",20:1,Dec/2023,2023,12,5.00,Google DeepMind,https://deepmind.google/technologies/gemini/,,35.7,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,Dense,"Bot, based on Chincilla",,,significant models
Gemini-1.5,75.8,google,22.4,1500,"30,000",20:1,Sep/2024,2024,9,5.75,Google DeepMind,https://aistudio.google.com/app/prompts/new_chat,75.8,59.1,https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/,MoE,,,YES,significant models
Gemma,64.3,google,0.7,7,"6,000",858:1,Feb/2024,2024,2,5.17,Google DeepMind,https://labs.pplx.ai/,33.7,,https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,Dense,,,,
Gemma 2,75.2,google,2,27,"13,000",482:1,Jun/2024,2024,6,5.50,Google DeepMind,https://huggingface.co/google/gemma-2-27b-it,,,https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,Dense,,,,
GLM-4,81.5,chinese,3,200,"4,000",20:1,Jan/2024,2024,1,5.08,Tsinghua,https://open.bigmodel.cn/,,,https://pandaily.com/zhipu-ai-unveils-glm-4-model-with-advanced-performance-paralleling-gpt-4/,Dense,Best Chinese model to date based on analysis. Follows OpenAI roadmap,,,significant models
Gopher,60.0,google,1.0,280,300,2:1,Dec/2021,2021,12,3.00,DeepMind,,,,https://arxiv.org/abs/2112.11446,Dense,,,,
GPT-2,32.4,openAI,0.0,1.5,10,7:1,Feb/2019,2019,2,0.17,OpenAI,Hugging Face,,,https://openai.com/blog/better-language-models/,Dense,trained on Reddit only,YES,,significant models
GPT-3,43.9,openAI,0.8,175,300,2:1,May/2020,2020,5,2.65,OpenAI,,,,https://arxiv.org/abs/2005.14165,Dense,,,,significant models
GPT-4 Classic,86.4,openAI,15.9,1760,"13,000",8:1,Mar/2023,2023,3,4.25,OpenAI,https://chat.openai.com/,,35.7,https://cdn.openai.com/papers/gpt-4.pdf,MoE,,,,significant models
GPT-4 Turbo*,86.4,openAI,,,"13,000",,Nov/2023,2023,11,4.92,OpenAI,https://chat.openai.com/,,46.5,https://cdn.openai.com/papers/gpt-4.pdf,MoE,significantly better model than other earlier GPTs,,,significant models
GPT-4o mini*,82.0,openAI,1.1,8,"13,000","1,625:1",Jul/2024,2024,7,5.58,OpenAI,https://chatgpt.com/,,40.2,https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,MoE,"""OpenAI would not disclose exactly how large GPT-4o mini is, but said it’s roughly in the same tier as other small AI models, such as Llama 3 8b, Claude Haiku and Gemini 1.5 Flash.""",,,significant models
GPT-4o*,88.7,openAI,6.7,200,"20,000",100:1,May/2024,2024,5,5.42,OpenAI,https://chatgpt.com/,72.6,53.6,https://openai.com/index/gpt-4o-system-card/,MoE,"likely early ""beta"" of GPT-5",,YES,significant models
GPT-NeoX,33.6,other,,20,,,Mar/2023,2023,3,4.25,Together,https://huggingface.co/spaces/togethercomputer/OpenChatKit,,,https://github.com/togethercomputer/OpenChatKit,Dense,,YES,,
Granite,57.0,other,0.6,13,"2,500",193:1,Sep/2023,2023,9,4.75,IBM,https://www.ibm.com/granite,,,https://www.ibm.com/downloads/cas/X9W4O6BM,,trained on 2.5T token,,,
Griffin,49.5,google,0.2,14,300,22:1,Feb/2024,2024,2,5.17,Google DeepMind,,,,https://arxiv.org/abs/2402.19427,Dense,,,,
GRIN MoE,79.4,microsoft,1.6,60,"4,025",68:1,Sep/2024,2024,9,5.75,Microsoft,https://huggingface.co/microsoft/GRIN-MoE,,,https://huggingface.co/microsoft/GRIN-MoE/blob/main/GRIN_MoE.pdf,MoE,,,,
Grok-1.5,81.3,other,4.6,314,"6,000",20:1,Mar/2024,2024,3,5.25,xAI,https://grok.x.ai/,,,https://x.ai/blog/grok-1.5,MoE,"Twitter chatbot trained on Twitter data, Context=128k.",,,
Grok-2,87.5,other,10.0,600,"15,000",25:1,Aug/2024,2024,8,5.67,xAI,https://x.com/i/grok,75.5,56,https://x.ai/blog/grok-2,MoE,Twitter chatbot trained on Twitter data,,YES,significant models
H20-Danube3-4B,55.2,other,0.5,4,"6,000","1,500:1",Jul/2024,2024,7,5.58,H20.ai,https://h2o.ai/platform/danube/personal-gpt/,,,https://arxiv.org/abs/2407.09276,Dense,"""Runs natively and fully offline on mobile phone.""",,,
Hawk,35.0,google,0.2,7,300,43:1,Feb/2024,2024,2,5.17,Google DeepMind,,,,https://arxiv.org/abs/2402.19427,Dense,,,,
HLAT,41.3,other,0.4,7,"1,800",258:1,Apr/2024,2024,4,5.33,Amazon,,,,https://arxiv.org/abs/2404.10630,Dense,,,,
iFlytekSpark-13B,63.0,other,0.7,13,"3,000",231:1,Jan/2024,2024,1,5.08,iFlyTek,https://gitee.com/iflytekopensource/iFlytekSpark-13B,,,https://www.ithome.com/0/748/030.htm,Dense,,,,
Inflection-2.5,85.5,other,16.3,1200,"20,000",17:1,Mar/2024,2024,3,5.25,Inflection AI,https://inflection.ai/inflection-2,,38.4,https://inflection.ai/inflection-2-5,Dense,,,,
InternLM2,67.7,chinese,0.8,20,"2,600",130:1,Jan/2024,2024,1,5.08,Shanghai AI Laboratory/SenseTime,https://github.com/InternLM/InternLM,,,https://arxiv.org/abs/2403.17297,Dense,Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS,,,
InternLM2.5,73.5,chinese,0.8,20,"2,600",130:1,Jul/2024,2024,7,5.58,Shanghai AI Laboratory/SenseTime,https://huggingface.co/internlm/internlm2_5-20b-chat,,38.4,https://github.com/InternLM/InternLM/blob/main/model_cards/internlm2.5_7b.md,Dense,Image-aware Decoder Enhanced à la Flamingo with Interleaved Cross-attentionS,YES,,
Jamba 1,67.4,other,1.7,52,"5,000",97:1,Mar/2024,2024,3,5.25,AI21,https://huggingface.co/ai21labs/Jamba-v0.1,,,https://arxiv.org/abs/2403.19887,MoE,,,,
Jamba 1.5,81.2,other,5.9,398,"8,000",21:1,Aug/2024,2024,8,5.67,AI21,https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251,53.5,36.9,https://arxiv.org/abs/2408.12570,MoE,"optimized for business use cases and capabilities such as function calling, structured output (JSON), and grounded generation.",,,
JetMoE-8B,49.2,other,0.3,8,"1,250",157:1,Apr/2024,2024,4,5.33,MIT,https://www.lepton.ai/playground/chat?model=jetmoe-8b-chat,,,https://huggingface.co/jetmoe/jetmoe-8b,MoE,,,,
K2,64.8,other,1.0,65,"1,400",22:1,May/2024,2024,5,5.42,LLM360,https://huggingface.co/LLM360/K2,,,https://www.llm360.ai/blog/several-new-releases-to-further-our-mission.html,Dense,outperforming Llama 2 70B using 35% less compute.,,,
LFM-40B,78.8,other,0.9,40,"2,000",50:1,Sep/2024,2024,9,5.75,Liquid AI,https://labs.perplexity.ai/,55.6,,https://www.liquid.ai/liquid-foundation-models,MoE,,,,
Llama 2,68.9,meta,1.2,70,"2,000",29:1,Jul/2023,2023,8,4.67,Meta AI,https://www.llama2.ai/,37.5,26.26,https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/,Dense,"Open source LLM comes in 3 parameter sizes - 7, 30, and 70 bn, Context window=4096",YES,,significant models
Llama 3 70B,82.0,meta,3.4,70,"15,000",215:1,Apr/2024,2024,4,5.33,Meta AI,https://meta.ai/,52.8,,https://ai.meta.com/blog/meta-llama-3/,Dense,,YES,,significant models
Llama 3.1 405B,88.6,meta,8.2,405,"15,000",38:1,Jul/2024,2024,7,5.58,Meta AI,https://www.meta.ai/,73.3,51.1,https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,Dense,,YES,YES,significant models
Llama 3.2 3B,63.4,meta,0.6,3.21,"9,000","2,804:1",Sep/2024,2024,9,5.75,Meta AI,https://www.llama.com/,,32.8,https://www.llama.com/,Dense,,,YES,significant models
LLaMA-65B,68.9,meta,1.0,65,"1,400",22:1,Feb/2023,2023,2,4.17,Meta AI,Weights leaked: https://github.com/facebookresearch/llama/pull/73/files ,,,https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/,Dense,"Researchers only, noncommercial only.",YES,,
Mamba,26.2,other,0.1,2.8,300,108:1,Dec/2023,2023,12,5.00,CMU,https://huggingface.co/havenhq/mamba-chat,,,https://arxiv.org/abs/2312.00752,Dense,,,,
MAP-Neo,58.1,other,0.6,7,"4,500",643:1,May/2024,2024,5,5.42,International,https://map-neo.github.io/,,,https://arxiv.org/abs/2405.19327,Dense,,YES,,
Minitron-4B,58.6,other,0.1,4,94,24:1,Aug/2024,2024,8,5.67,NVIDIA,https://huggingface.co/nvidia/Minitron-4B-Base,,,https://arxiv.org/abs/2407.14679,Dense,,,,
Minitron-8B,63.8,other,0.1,4,94,24:1,Jul/2024,2024,7,5.58,NVIDIA,https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base,,,https://blogs.nvidia.com/blog/mistral-nemo-minitron-8b-small-language-model/,Dense,,,,significant models
Mistral 7B,30.9,mistral,0.3,7.3,800,110:1,Sep/2023,2023,9,4.75,Mistral,https://huggingface.co/mistralai,,,https://mistral.ai/news/announcing-mistral-7b/,,"Open source, outperforms Llama2",YES,,significant models
Mistral Large,81.2,mistral,5.2,300,"8,000",27:1,Feb/2024,2024,2,5.17,Mistral,https://poe.com/Mistral-Large,,,https://mistral.ai/news/mistral-large/,Dense,"natively fluent in English, French, Spanish, German, and Italian",,,significant models
Mistral Large 2,84.0,mistral,3.3,123,"8,000",66:1,Jul/2024,2024,7,5.58,Mistral,https://huggingface.co/mistralai/Mistral-Large-Instruct-2407,,,https://mistral.ai/news/mistral-large-2407/,Dense,,,,significant models
Mistral Small,72.2,mistral,0.5,7,"3,000",429:1,Feb/2024,2024,2,5.17,Mistral,https://chat.mistral.ai/chat,,,https://mistral.ai/news/mistral-large/,Dense,Optimised for latency and cost.,,,significant models
Mistral-medium,75.3,mistral,2.6,180,"3,500",20:1,Dec/2023,2023,12,5.00,Mistral,https://poe.com/,,,https://mistral.ai/news/la-plateforme/,Dense,,,,
mixtral-8x22b,77.8,mistral,1.8,141,"2,000",15:1,Apr/2024,2024,4,5.33,Mistral,https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1,,,https://mistral.ai/news/mixtral-8x22b/,MoE,,,,
mixtral-8x7b-32kseqlen,70.6,mistral,2.0,46.7,"8,000",172:1,Dec/2023,2023,12,5.00,Mistral,https://www.together.ai/blog/mixtral,43.3,,https://arxiv.org/abs/2401.04088,MoE,processes input and generates output at the same speed and for the same cost as a 12B model.',,,
NeMo,68.0,mistral,0.5,12,"2,000",167:1,Jul/2024,2024,7,5.58,Mistral,https://huggingface.co/mistralai/Mistral-Nemo-Base-2407,,,https://mistral.ai/news/mistral-nemo/,Dense,,,,
Nemotron-3 22B,54.4,other,1.0,22,"3,800",173:1,Nov/2023,2023,11,4.92,NVIDIA,https://huggingface.co/nvidia/nemotron-3-8b-base-4k,,,https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/,Dense,Nvidia's LLM,,,
Nemotron-4 15B,64.2,other,1.2,15,"8,000",534:1,Feb/2024,2024,2,5.17,NVIDIA,,,,https://arxiv.org/abs/2402.16819,Dense,Nvidia's LLM,,,
Nemotron-4-340B,81.1,other,5.8,340,"9,000",27:1,Jun/2024,2024,6,5.50,NVIDIA,https://build.nvidia.com/nvidia/nemotron-4-340b-instruct,,,https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T.pdf,Dense,Nvidia's LLM. Open-source equiv of Mar/2023 GPT-4,YES,YES,significant models
NLVM 1.0,82.0,other,3.8,72,"18,000",250:1,Sep/2024,2024,9,5.75,NVIDIA,https://huggingface.co/nvidia/NVLM-D-72B,,,https://arxiv.org/abs/2409.11402,Dense,Flamingo clone.,,,
o1*,92.3,openAI,6.7,200,"20,000",100:1,Sep/2024,2024,9,5.75,OpenAI,https://chatgpt.com/,91,78.3,https://openai.com/index/introducing-openai-o1-preview/,MoE,,,YES,significant models
OLMoE-1B-7B,54.1,other,0.7,6.9,"5,900",856:1,Sep/2024,2024,9,5.75,Allen AI,https://huggingface.co/collections/allenai/olmoe-66cf678c047657a30c8cd3da,,23,https://arxiv.org/abs/2409.02060v1,MoE,,,,
OpenELM,26.8,other,0.2,3.04,"1,500",494:1,Apr/2024,2024,4,5.33,Apple,https://huggingface.co/apple/OpenELM-3B-Instruct,,,https://arxiv.org/abs/2404.14619,Dense,,,,
Orion-14B,69.6,other,0.6,14,"2,500",179:1,Jan/2024,2024,1,5.08,OrionStar,https://github.com/OrionStarAI/Orion,,,https://arxiv.org/abs/2401.12246,Dense,"English, Chinese, Japanese, Korean, and other languages. ",,,
Palmyra X,70.2,other,1.0,72,"1,200",17:1,Jan/2024,2024,1,5.08,Writer,,,,https://writer.com/blog/palmyra-helm-benchmark/,Dense,,,,
phi-3-medium,78.2,microsoft,0.9,14,"4,800",343:1,Apr/2024,2024,4,5.33,Microsoft,https://huggingface.co/microsoft/Phi-3-medium-128k-instruct,55.7,,https://arxiv.org/abs/2404.14219,Dense,,,,
phi-3-mini,68.8,microsoft,0.4,3.8,"3,300",869:1,Apr/2024,2024,4,5.33,Microsoft,https://huggingface.co/microsoft/Phi-3-mini-128k-instruct,45.7,,https://arxiv.org/abs/2404.14219,Dense,phi3-mini can be quantized to 4-bits so that it only occupies ≈ 1.8GB of memory.,,,
phi-3.5-mini,69.0,microsoft,0.4,3.8,"3,400",895:1,Aug/2024,2024,8,5.67,Microsoft,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,47.4,30.4,https://arxiv.org/abs/2407.13833,Dense,,,,
phi-3.5-MoE,78.9,microsoft,1.8,60,"4,900",82:1,Aug/2024,2024,8,5.67,Microsoft,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,54.3,36.8,https://arxiv.org/abs/2407.13833https://arxiv.org/abs/2407.13833,MoE,,,,
Pile-T5,53.8,other,0.5,11,"2,000",182:1,Apr/2024,2024,4,5.33,EleutherAI,https://huggingface.co/EleutherAI/pile-t5-xxl,,,https://blog.eleuther.ai/pile-t5/,Dense,,,,
Pixtral-12b-240910,69.2,mistral,0.9,12,"6,000",500:1,Sep/2024,2024,9,5.75,Mistral,https://huggingface.co/mistralai/Pixtral-12B-2409,,,https://mistral.ai/news/pixtral-12b/,Dense,,,,
Qwen-1.5 110B,80.4,chinese,1.9,111,"3,000",28:1,Apr/2024,2024,4,5.33,Alibaba,https://huggingface.co/spaces/Qwen/Qwen1.5-110B-Chat-demo,49.9,35.9,https://qwenlm.github.io/blog/qwen1.5-110b/,Dense,,,,
Qwen1.5-MoE-A2.7B,62.5,chinese,0.5,14.3,"1,500",105:1,Mar/2024,2024,3,5.25,Alibaba,https://qwenlm.github.io/blog/qwen-moe/,,,https://qwenlm.github.io/blog/qwen-moe/,MoE,,,,
Qwen2,84.2,chinese,2.4,72,"7,000",98:1,Jun/2024,2024,6,5.50,Alibaba,https://huggingface.co/spaces/Qwen/Qwen2-72B-Instruct,55.6,37.9,https://arxiv.org/abs/2407.10671,Dense,,,,
Qwen2.5,86.1,chinese,3.8,72,"18,000",250:1,Sep/2024,2024,9,5.75,Alibaba,https://huggingface.co/Qwen/Qwen2.5-72B-Instruct,71.1,49,https://qwenlm.github.io/blog/qwen2.5/,Dense,,,YES,significant models
RakutenAI-7B,61.3,other,0.5,7,"3,000",429:1,Mar/2024,2024,3,5.25,Rakuten Group,https://huggingface.co/Rakuten/RakutenAI-7B,,,https://arxiv.org/abs/2403.15484,Dense,,,,
Reka Core,83.2,other,5.8,300,"10,000",34:1,Apr/2024,2024,4,5.33,Reka AI,https://poe.com/RekaCore,,38.2,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,,,,significant models
Reka Edge,63.1,other,0.6,7,"4,500",643:1,Feb/2024,2024,2,5.17,Reka AI,https://chat.reka.ai/,,,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,,,,
Reka Flash,73.5,other,1.1,21,"5,000",239:1,Feb/2024,2024,2,5.17,Reka AI,https://poe.com/RekaFlash,,34,https://publications.reka.ai/reka-core-tech-report.pdf,Dense,,,,
Rene,32.6,other,0.1,1.3,"1,500","1,154:1",Aug/2024,2024,8,5.67,Cartesia,https://huggingface.co/cartesia-ai/Rene-v0.1-1.3b-pytorch,,,https://cartesia.ai/blog/2024-08-27-on-device,Dense,,,,
RoBERTa,27.9,meta,0.1,0.355,"2,200","6,198:1",Jul/2019,2019,7,2.65,Meta AI,Hugging Face,,,https://arxiv.org/abs/1907.11692,Dense,,,,
RWKV-v5 Eagle 7B,33.2,other,0.3,7.52,"1,100",147:1,Jan/2024,2024,1,5.08,RWKV,https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2,,,https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers,Dense,,,,
RWKV-v5 EagleX,40.1,other,0.4,7.52,"1,700",227:1,Mar/2024,2024,3,5.25,RWKV,https://huggingface.co/recursal/EagleX_1-7T,,,https://substack.recursal.ai/p/eaglex-17t-soaring-past-llama-7b,Dense,,,,
SenseNova 5.0,84.8,chinese,8.2,600,"10,000",17:1,Apr/2024,2024,4,5.33,SenseTime,,,42.93,https://news.futunn.com/en/post/41290101/a-large-shangtang-multi-modal-model-with-600-billion-parameters,MoE,GPT-4 scale; low media coverage; no demo in Western world,,,
Skywork MoE 16x13B,77.4,chinese,,146,,,Jun/2024,2024,6,5.50,Kunlun Tech,https://huggingface.co/Skywork/Skywork-MoE-Base,,,https://github.com/SkyworkAI/Skywork-MoE/blob/main/skywork-moe-tech-report.pdf,MoE,,,,
Skywork-13B,62.7,chinese,0.7,13,"3,200",247:1,Oct/2023,2023,10,4.83,Kunlun Tech,,,,https://arxiv.org/abs/2310.19341,Dense,,,,
Tele-FLM,64.0,chinese,1.1,52,"2,000",39:1,Apr/2024,2024,4,5.33,BAAI,https://huggingface.co/CofeAI/Tele-FLM,,,https://arxiv.org/abs/2404.16645,Dense,,,,
TeleChat2-115B,80.9,other,3.6,115,"10,000",87:1,Sep/2024,2024,9,5.75,China Telecom Artificial Intelligence Research Institute,https://modelscope.cn/models/TeleAI/TeleChat2-115B,,,https://github.com/Tele-AI/TeleChat2,Dense,Trained on Chinese GPUs,,,
Titan,70.4,other,3.0,200,"4,000",20:1,Apr/2023,2023,4,4.33,Amazon,https://aws.amazon.com/bedrock/titan/,,,https://www.techrepublic.com/article/amazon-bedrock-titan-cloud-artificial-intelligence/,Dense,,,,significant models
U-PaLM,74.1,google,2.2,540,780,2:1,Oct/2022,2022,10,3.83,Google,,,,https://arxiv.org/abs/2210.11399,Dense,,,,
UL2 20B,39.2,google,0.5,20,"1,000",50:1,May/2022,2022,5,3.42,Google,,,,https://arxiv.org/abs/2205.05131,,,,,
Xinghuo 3.5,86,chinese,3.0,200,"4,000",20:1,Jan/2024,2024,1,5.08,iFlyTek,,,,https://www.shine.cn/biz/tech/2401304331/,Dense,"""better than GPT-4""",,,
YAYI 2,80.5,chinese,0.9,30,"2,650",89:1,Dec/2023,2023,12,5.00,Wenge,https://huggingface.co/wenge-research/yayi2-30b,,,https://arxiv.org/abs/2312.14862,Dense,,,,
Yi 1.5 34B,76.8,chinese,1.2,34.4,"3,600",105:1,May/2024,2024,5,5.42,01-ai,https://huggingface.co/01-ai/Yi-1.5-34B-Chat,52.3,,https://github.com/01-ai/Yi-1.5,Dense,,,,
Yi-34B,76.3,chinese,1.1,34.4,"3,000",88:1,Nov/2023,2023,11,4.92,01-ai,https://huggingface.co/01-ai/Yi-34B,43,,https://github.com/01-ai/Yi,Dense,Outperforms Llama 2. Chinese and English.,,,significant models
Yi-Large,83.8,chinese,12.9,1000,"15,000",15:1,May/2024,2024,5,5.42,01-ai,https://platform.01.ai/,58.1,43.5,https://www.aixinzhijie.com/article/6845768,Dense,,,,
Yi-XLarge,85.1,chinese,21.1,2000,"20,000",10:1,May/2024,2024,5,5.42,01-ai,https://platform.01.ai/,,48.2,https://www.aixinzhijie.com/article/6845768,MoE,,,,
Nova Pro*,88.8,other,,,,,May/2024,2024,5,5.42,Rubiks AI,https://rubiks.ai/,,,https://rubiks.ai/nova/release/,Dense,,,,