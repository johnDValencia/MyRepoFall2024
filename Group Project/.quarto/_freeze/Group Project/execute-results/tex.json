{
  "hash": "45f9f97b41698294557e2fad2957e135",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Group Project - Data Visualization Recreation\"\nauthor: John D. Valencia\nformat: pdf\n---\n\n\n## Quarto\n\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggrepel) # For better label placement\n```\n:::\n\n\nLoad in dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the dataset\nfile_path <- \"IIB LLMs public (new Oct 2024) - LLMs-for-VZ.csv\"\nllms <- read_csv(file_path)\n\n# View the structure and first few rows of the dataset\nstr(llms)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nspc_tbl_ [123 x 21] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Model               : chr [1:123] \"source: LifeArchitect\\nhttps://docs.google.com/spreadsheets/d/1kc262HZSMAWI6FVsh0zJwbB-ooYvzhCHaHcNUiA0_hY/edit\"| __truncated__ \"AMD-Llama-135m\" \"Apple On-Device Jun 24\" \"Arctic\" ...\n $ MMLU                : num [1:123] NA 23 26.8 67.3 47.9 54.2 39.1 39.2 44 44 ...\n $ creator             : chr [1:123] NA \"other\" \"other\" \"other\" ...\n $ AL score            : chr [1:123] \"ALScore \\n\\\"ALScore\\\" is a quick and dirty rating of the model's power. The formula is:\\nSqr Root of (Parameter\"| __truncated__ \"0.0\" \"0.2\" \"4.3\" ...\n $ Parameters \n(Bn)   : num [1:123] NA 0.135 3.04 480 11 13 176 50 175 530 ...\n $ Tokens \ntrained (B): num [1:123] NA 670 1500 3500 40 2600 366 569 300 300 ...\n $ Ratio Tokens        : chr [1:123] \"Ratio Tokens:Params\\n(Chinchilla scalingâ‰¥20:1)\" \"4,963:1\" \"494:1\" \"8:1\" ...\n $ Announced           : chr [1:123] NA \"Sep/2024\" \"Jun/2024\" \"Apr/2024\" ...\n $ year                : num [1:123] NA 2024 2024 2024 2022 ...\n $ month               : num [1:123] NA 9 6 4 8 9 7 3 2 2 ...\n $ date                : chr [1:123] \"as numeric\" \"5.75\" \"5.50\" \"5.33\" ...\n $ Lab                 : chr [1:123] NA \"AMD\" \"Apple\" \"Snowflake AI Research\" ...\n $ Playground          : chr [1:123] NA \"https://huggingface.co/amd/AMD-Llama-135m\" \"https://github.com/apple/corenet/tree/main/projects/openelm\" \"https://arctic.streamlit.app/\" ...\n $ MMLU\n-Pro          : num [1:123] NA NA NA NA NA NA NA NA NA NA ...\n $ GPQA                : num [1:123] NA NA NA NA NA NA NA NA NA NA ...\n $ Link                : chr [1:123] NA \"https://www.amd.com/en/developer/resources/technical-articles/introducing-amd-first-slm-135m-model-fuels-ai-advancements.html\" \"https://arxiv.org/abs/2404.14619\" \"https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/\" ...\n $ Archiecture         : chr [1:123] NA \"Dense\" \"Dense\" \"Hybrid\" ...\n $ Note                : chr [1:123] NA \"Small language model (SLM) trained on 70,000 open access books\" NA NA ...\n $ open access         : chr [1:123] NA NA NA NA ...\n $ force label         : chr [1:123] NA NA \"YES\" NA ...\n $ show only           : chr [1:123] NA NA \"significant models\" NA ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Model = col_character(),\n  ..   MMLU = col_double(),\n  ..   creator = col_character(),\n  ..   `AL score` = col_character(),\n  ..   `Parameters \n  .. (Bn)` = col_double(),\n  ..   `Tokens \n  .. trained (B)` = col_number(),\n  ..   `Ratio Tokens` = col_character(),\n  ..   Announced = col_character(),\n  ..   year = col_double(),\n  ..   month = col_double(),\n  ..   date = col_character(),\n  ..   Lab = col_character(),\n  ..   Playground = col_character(),\n  ..   `MMLU\n  .. -Pro` = col_double(),\n  ..   GPQA = col_double(),\n  ..   Link = col_character(),\n  ..   Archiecture = col_character(),\n  ..   Note = col_character(),\n  ..   `open access` = col_character(),\n  ..   `force label` = col_character(),\n  ..   `show only` = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(llms)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 21\n  Model       MMLU creator `AL score` `Parameters \\n(Bn)` `Tokens \\ntrained (B)`\n  <chr>      <dbl> <chr>   <chr>                    <dbl>                  <dbl>\n1 \"source: ~  NA   <NA>    \"ALScore ~              NA                         NA\n2 \"AMD-Llam~  23   other   \"0.0\"                    0.135                    670\n3 \"Apple On~  26.8 other   \"0.2\"                    3.04                    1500\n4 \"Arctic\"    67.3 other   \"4.3\"                  480                       3500\n5 \"Atlas\"     47.9 meta    \"0.1\"                   11                         40\n6 \"Baichuan~  54.2 chinese \"0.6\"                   13                       2600\n# i 15 more variables: `Ratio Tokens` <chr>, Announced <chr>, year <dbl>,\n#   month <dbl>, date <chr>, Lab <chr>, Playground <chr>, `MMLU\\n-Pro` <dbl>,\n#   GPQA <dbl>, Link <chr>, Archiecture <chr>, Note <chr>, `open access` <chr>,\n#   `force label` <chr>, `show only` <chr>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Rename specific columns in the llms dataframe\nllms <- llms %>%\n  rename(\n    parameters_bn = `Parameters \\n(Bn)`,            # Clean name\n    tokens_trained_B = `Tokens \\ntrained (B)`,     # Clean name\n    MMLU_Pro = `MMLU\\n-Pro`                        # Clean name\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clean and prepare data\nllms <- llms %>%\n  filter(!is.na(MMLU), !is.na(year), !is.na(parameters_bn)) %>% # Remove rows with NA in important columns\n  mutate(\n    creator = as.factor(creator), # Convert creator to a factor\n    year = as.numeric(year),\n    MMLU = as.numeric(MMLU),\n    parameters_bn = as.numeric(parameters_bn)\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove rows with NA in the date column and ensure it's numeric\nllms <- llms %>%\n  filter(!is.na(date)) %>%\n  mutate(date = as.numeric(date))\n\n# Summary of date\nsummary(llms$date)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.17    5.00    5.25    5.05    5.50    5.75 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Combine year and month into a Date column (assuming day = 1)\nllms <- llms %>%\n  mutate(\n    date_as_date = as.Date(paste0(year, \"-\", sprintf(\"%02d\", month), \"-01\"))\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine month and year into a new column\nllms <- llms %>%\n  mutate(\n    month_year = paste0(year, \"-\", sprintf(\"%02d\", month)) # Create a \"YYYY-MM\" format\n  )\n\n# Convert month_year to a factor ordered by chronological appearance\nllms <- llms %>%\n  mutate(\n    month_year = factor(month_year, levels = unique(month_year[order(year, month)]))\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check unique year values\nunique(llms$year)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2024 2022 2023 2021 2019 2020\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check if earlier years have data\nllms %>%\n  filter(year < 2024) %>%\n  select(year, month, month_year) %>%\n  arrange(year, month)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 33 x 3\n    year month month_year\n   <dbl> <dbl> <fct>     \n 1  2019     2 2019-02   \n 2  2019     7 2019-07   \n 3  2020     5 2020-05   \n 4  2021    12 2021-12   \n 5  2022     3 2022-03   \n 6  2022     5 2022-05   \n 7  2022     7 2022-07   \n 8  2022     8 2022-08   \n 9  2022    10 2022-10   \n10  2022    10 2022-10   \n# i 23 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define x_limit_min and x_breaks for pre-2022 and post-2021 years\nx_limit_min <- as.Date(\"2019-01-01\")\nx_limit_max <- max(llms$date_as_date, na.rm = TRUE)\nall_years_post2021 <- 2022:max(llms$year, na.rm = TRUE)\nx_breaks <- c(as.Date(\"2021-01-01\"), as.Date(paste0(all_years_post2021, \"-01-01\")))\nx_labels <- c(\"pre-2022\", as.character(all_years_post2021))\n\n\nllms <- llms %>%\n  mutate(\n    source = case_when(\n      `open access` == \"YES\" ~ \"Open\",\n      TRUE ~ \"Closed\"\n    ),\n    date_label = ifelse(date_as_date < as.Date(\"2022-01-01\"), \"pre-2022\", as.character(year(date_as_date)))\n  )\n\n\n# Plot\nggplot(llms, aes(x = date_as_date, \n                 y = MMLU, \n                 size = parameters_bn, \n                 color = creator, \n                 shape = source)) +\n  geom_point(alpha = 0.7) +\n  \n  # Add labels only for models with significance\n  geom_text(\n    data = subset(llms, `force label` == \"YES\" | (!is.na(Note) & Note != \"\") | `show only` == \"significant models\"),\n    aes(label = Model),\n    color = \"black\",\n    vjust = 1.5,\n    size = 3\n  ) +\n  \n  # Add horizontal benchmark lines\n  geom_hline(yintercept = 70, linetype = \"dashed\", color = \"red\") +\n  geom_hline(yintercept = 89.8, linetype = \"dashed\", color = \"blue\") +\n  \n   # Adjust y-axis to ensure 100 MMLU is the final mark\n  scale_y_continuous(\n    name = \"MMLU Benchmark Score\",\n    limits = c(18, 100),  # Set the range from 0 to 100\n    breaks = seq(0, 100, by = 20)  # Customize breaks (0, 20, 40, ..., 100)\n  ) +\n  \n  # Add labels for the benchmark lines\n  annotate(\"text\", \n           x = x_limit_min, \n           y = 70, \n           label = \"70+ IDEAL\", \n           hjust = 0, \n           vjust = 1.5, \n           color = \"red\") +\n  annotate(\"text\", \n           x = x_limit_min, \n           y = 89.8, \n           label = \"88.9 = human expert\", \n           hjust = 0, \n           vjust = 1.5, \n           color = \"blue\") +\n  \n  # Add a polynomial regression line\n  geom_smooth(\n    aes(group = 1),\n    method = \"lm\", \n    formula = y ~ poly(as.numeric(x), 5), # Convert Date to numeric and use degree 5\n    se = FALSE, \n    color = \"black\",\n    linetype = \"solid\",\n    size = 0.5\n  ) +\n  \n  scale_x_date(\n    name = \"Year\",\n    breaks = x_breaks,\n    labels = x_labels,\n    limits = c(x_limit_min, x_limit_max),\n    expand = expansion(mult = c(0.02, 0.02))\n  ) +\n  \n  scale_shape_manual(\n    values = c(\"Open\" = 18, \"Closed\" = 16), # Diamonds for Open, Circles for Closed\n    name = \"Source\"\n  ) +\n  \n  scale_size_continuous(\n    range = c(2, 9),  # Define size range for bubbles\n    labels = c(\"1B\", \"10B\", \"100B\", \"1T\", \"10T\")  # Customize legend labels\n  ) +\n  labs(\n    title = \"Major Large Language Models (LLMs)\",\n    subtitle = \"Ranked by capabilities, sized by billion parameters used for training\",\n    y = \"MMLU Benchmark Score\",\n    color = \"Creator\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 9),\n    axis.text.x = element_text(angle = 0, hjust = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12)\n  ) +\n  \n  guides(size = \"none\") + # Remove size legend\n  annotate(\"text\", \n           x = x_limit_min, \n           y = max(llms$MMLU, na.rm = TRUE), \n           label = \"Parameters = Size\", \n           hjust = 0, \n           vjust = 1, \n           size = 4, \n           color = \"black\")\n```\n\n::: {.cell-output-display}\n![](Group-Project_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(llms)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [115 x 25] (S3: tbl_df/tbl/data.frame)\n $ Model           : chr [1:115] \"AMD-Llama-135m\" \"Apple On-Device Jun 24\" \"Arctic\" \"Atlas\" ...\n $ MMLU            : num [1:115] 23 26.8 67.3 47.9 54.2 39.1 39.2 44 44 65.8 ...\n $ creator         : Factor w/ 8 levels \"anthropic\",\"chinese\",..: 8 8 8 4 2 8 8 2 2 4 ...\n $ AL score        : chr [1:115] \"0.0\" \"0.2\" \"4.3\" \"0.1\" ...\n $ parameters_bn   : num [1:115] 0.135 3.04 480 11 13 176 50 175 530 34 ...\n $ tokens_trained_B: num [1:115] 670 1500 3500 40 2600 366 569 300 300 9200 ...\n $ Ratio Tokens    : chr [1:115] \"4,963:1\" \"494:1\" \"8:1\" \"4:1\" ...\n $ Announced       : chr [1:115] \"Sep/2024\" \"Jun/2024\" \"Apr/2024\" \"Aug/2022\" ...\n $ year            : num [1:115] 2024 2024 2024 2022 2023 ...\n $ month           : num [1:115] 9 6 4 8 9 7 3 2 2 5 ...\n $ date            : num [1:115] 5.75 5.5 5.33 3.67 4.75 3.58 4.25 5.17 5.17 5.42 ...\n $ Lab             : chr [1:115] \"AMD\" \"Apple\" \"Snowflake AI Research\" \"Meta AI\" ...\n $ Playground      : chr [1:115] \"https://huggingface.co/amd/AMD-Llama-135m\" \"https://github.com/apple/corenet/tree/main/projects/openelm\" \"https://arctic.streamlit.app/\" NA ...\n $ MMLU_Pro        : num [1:115] NA NA NA NA NA NA NA NA NA NA ...\n $ GPQA            : num [1:115] NA NA NA NA NA NA NA NA NA NA ...\n $ Link            : chr [1:115] \"https://www.amd.com/en/developer/resources/technical-articles/introducing-amd-first-slm-135m-model-fuels-ai-advancements.html\" \"https://arxiv.org/abs/2404.14619\" \"https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/\" \"https://arxiv.org/abs/2208.03299\" ...\n $ Archiecture     : chr [1:115] \"Dense\" \"Dense\" \"Hybrid\" \"Dense\" ...\n $ Note            : chr [1:115] \"Small language model (SLM) trained on 70,000 open access books\" NA NA NA ...\n $ open access     : chr [1:115] NA NA NA NA ...\n $ force label     : chr [1:115] NA \"YES\" NA NA ...\n $ show only       : chr [1:115] NA \"significant models\" NA NA ...\n $ date_as_date    : Date[1:115], format: \"2024-09-01\" \"2024-06-01\" ...\n $ month_year      : Factor w/ 27 levels \"2019-02\",\"2019-07\",..: 27 24 22 8 15 7 12 20 20 23 ...\n $ source          : chr [1:115] \"Closed\" \"Closed\" \"Closed\" \"Closed\" ...\n $ date_label      : chr [1:115] \"2024\" \"2024\" \"2024\" \"2022\" ...\n```\n\n\n:::\n:::\n",
    "supporting": [
      "Group-Project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}